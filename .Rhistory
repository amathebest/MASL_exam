cv.lasso$lambda
lasso_minL <- cv.lasso$lambda.min
lasso_minL
fit.lasso <- glmnet(X, Y, alpha = 1)
min_idx_L <- which.min(abs(fit.lasso$lambda - cv.lasso$lambda.min))
min_idx_L
chosen.L <- which.min(abs(fit.lasso$lambda-cv.lasso$lambda.min))
chosen.L
plot(fit.lasso, xvar = "lambda", label = T)
abline(v = log(fit.lasso$lambda[min_idx_L]))
sum(fit.lasso$beta[,min_idx_L] == 0)
p <- ncol(X)
n <- nrow(X)
psim <- 20
p.zero <- p-psim
set.seed(110)
beta.vec <- c(sample(c(-.1, -.2, .1, .2), psim, replace = T), rep(0, (p-psim)))
beta.vec
beta.vec <- matrix(sample(beta.vec), ncol = 1)
beta.vec
beta.hat <- matrix(NA, nsim, p)
chosen <- rep(NA, nsim)
X <- scale(X)
for (i in 1:nsim) {
set.seed(110+i)
Ysim <- X%*%beta.vec + rnorm(n, 0, 0.5) # adding some error to the variables times the scenario coefficients
fit.lasso <- glmnet(X, Ysim, alpha = 1)
cv.lasso <- cv.glmnet(X, Ysim, nfolds = 10)
chosen[i] <- which.min(abs(fit.lasso$lambda - cv.lasso$lambda.min))
beta.hat[i,] <- fit.lasso$beta[,chosen[i]]
}
for (i in 1:nsim) {
set.seed(110+i)
Ysim <- X%*%beta.vec + rnorm(n, 0, 0.5) # adding some error to the variables times the scenario coefficients
fit.lasso <- glmnet(X, Ysim, alpha = 1)
cv.lasso <- cv.glmnet(X, Ysim, nfolds = 10)
chosen[i] <- which.min(abs(fit.lasso$lambda - cv.lasso$lambda.min))
beta.hat[i,] <- fit.lasso$beta[,chosen[i]]
}
round(colMeans(beta.hat), 4)
round(colMeans(beta.hat), 4)[beta.vec == 0]
round(apply(beta.hat, 2, var), 4)
plot(density(chosen))
round(apply(beta.hat, 2, var), 4)[beta.vec == 0]
plot(density(chosen))
beta.vec[(p-1):p]
plot(density(beta.hat[,p]))
plot(density(beta.hat[,p-1]))
plot(density(beta.hat[,p]))
zeros <- which(beta.vec == 0)
zeros
zeros <- sample(zeros, nshow)
nshow <- 20
zeros <- sample(zeros, nshow)
nshow <- 10
zeros <- sample(zeros, nshow)
col <- gray.colors(nshow)
plot(density(beta.hat[,altro[1]]), xlim=c(-.2,.5), ylim=c(0,30))
plot(density(beta.hat[,zeros[1]]), xlim=c(-.2,.5), ylim=c(0,30))
plot(density(beta.hat[,zeros[1]]), xlim=c(-.3,.3), ylim=c(0,30))
for (i in 2:length(altro)) {
lines(density(beta.hat[,altro[i]]), col=colore[i])
}
for (i in 2:length(zeros)) {
lines(density(beta.hat[,zeros[i]]), col=colore[i])
}
for (i in 2:length(zeros)) {
lines(density(beta.hat[,zeros[i]]), col=col[i])
}
data("bodyfat", package = "TH.data")
install.packages("TH.data")
data("bodyfat", package = "TH.data")
head(bodyfat)
library(glasso)
install.packages("glasso")
View(data)
BFmod0 <- glasso(cor(bodyfat), rho = 0.9)# modify rho
library(glasso)
BFmod0 <- glasso(cor(bodyfat), rho = 0.9)# modify rho
BFmod0 <- glasso(cor(bodyfat), nobs = nrow(bodyfat), rho = 0.9, penalize.diagonal = F)# modify rho
# takes var/cov matrix, and rho -> lambda, reg. estimator.
BFmod0$loglik
round(BFmod0$wi, 3)
data("bodyfat", package = "TH.data")
head(bodyfat)
BFmod0 <- glasso(cor(bodyfat), nobs = nrow(bodyfat), rho = 0.9, penalize.diagonal = F)# modify rho
# takes var/cov matrix, and rho -> lambda, reg. estimator.
BFmod0$loglik
round(BFmod0$wi, 3)
BFmod0 <- glasso(cor(bodyfat), nobs = nrow(bodyfat), rho = 0.5, penalize.diagonal = F)# modify rho
# takes var/cov matrix, and rho -> lambda, reg. estimator.
BFmod0$loglik
round(BFmod0$wi, 3) # we put the shrinkage on the off-diagonal elements of the inverse of the variance/covariance matrix
grafo <- matrix(as.numeric(BFmod0$wi !=0), nrow = 10)
grafo
diag(grafo) <- 0
drawGraph(grafo)
library(ggm)
colnames(grafo) <- rownames(grafo) <- colnames(bodyfat)
drawGraph(grafo)
library(glasso)
data("bodyfat", package = "TH.data")
head(bodyfat)
BFmod0 <- glasso(cor(bodyfat), nobs = nrow(bodyfat), rho = 0.5, penalize.diagonal = F)# modify rho
# takes var/cov matrix, and rho = lambda, reg. estimator.
BFmod0$loglik
round(BFmod0$wi, 3) # we put the shrinkage on the off-diagonal elements of the inverse of the variance/covariance matrix
grafo <- matrix(as.numeric(BFmod0$wi !=0), nrow = 10)
grafo
diag(grafo) <- 0
library(ggm)
colnames(grafo) <- rownames(grafo) <- colnames(bodyfat)
drawGraph(grafo)
stima <- ggm:fitConGraph(grafo, S = var(bodyfat), n = 71)
stima <- ggm::fitConGraph(grafo, S = var(bodyfat), n = 71)
stima
library(gRbase)
library(gRain)
library(Rgraphviz)
#### How to construct a DAG
dag0 <- dag(~a, ~b * a, ~c * a * b, ~d * c * e, ~e * a)
edgeList(dag0) # when the class of edges is "oriented", every edge is in the form of <parent> <child>
#### How to construct a DAG
dag0 <- dag(~a, ~b * a, ~c * a * b, ~d * c * e, ~e * a)
Rgraphviz::plot(dag0, cex.main = 0.5) # note how the script works: ~c * a * b = c depends on both a and b
dag0
edgeList(dag0) # when the class of edges is "oriented", every edge is in the form of <parent> <child>
as.adjMAT(dag0) # adjacency matrix, on rows = parents, on columns = child
# these two functions identify the parents or the children of a specified node
parents("d", dag0)
children("a", dag0)
# check: b ind e?
ancestralSet(c("b", "e"), dag0)
iplot(ancestralGraph(c("b", "e"), dag0))
iplot(moralize(ancestralGraph(c("b", "e"), dag0))) # --> no
library(gRbase)
library(gRain)
library(Rgraphviz)
#### How to construct a DAG
dag0 <- dag(~a*b,~a*c,~c*b,~b*d,~a*e,~e*d,~c*f,~b*f)
Rgraphviz::plot(dag0, cex.main = 0.5) # note how the script works: ~c * a * b = c depends on both a and b
#### How to construct a DAG
dag0 <- dag(~b*a,~c*a,~b*c,~d*b,~e*a,~d*e,~f*c,~f*b)
Rgraphviz::plot(dag0, cex.main = 0.5) # note how the script works: ~c * a * b = c depends on both a and b
edgeList(dag0) # when the class of edges is "oriented", every edge is in the form of <parent> <child>
# check: b ind e?
ancestralSet(c("c"), dag0)
#########  Data genetation
# change seetting as you prefer
n <- 200 # change to 100, 500
set.seed(26)
X1 <- round(rnorm(n),1)
X2 <- round(rt(n, df=6), 1)
X3 <- rbinom(n,1,.6) # attention to set the factors if not binary
Y <- X1^2 - 2*X1*X2 + 2*X1^X3 - X1*I(X2>0) + round(rnorm(n),1)
#########  Data genetation
# change seetting as you prefer
n <- 500 # change to 100, 500
set.seed(26)
X1 <- round(rnorm(n), 1)
X2 <- round(rt(n, df = 6), 1) # t distribution
X3 <- rbinom(n, 1, .6) # attention to set the factors if not binary, bernoulli distribution
Y <- X1^2 - 2*X1*X2 + 2*X1^X3 - X1*I(X2>0) + round(rnorm(n),1)
hist(X1)
hist(X2)
hist(x3)
hist(X3)
e <- round(rnorm(n), 1)
Y <- X1^2 - 2*X1*X2 + 2*X1^X3 - X1*I(X2>0) + e
plot(Y)
plot(density(Y))
## ordinary regression
mod0 <- lm(Y ~ X1 + X2 + X3)
summary(mod0)
# this is the true regression model, assumed hierarchical.
# use : instead of * to have exactly the true model
mod0.true <- lm(Y~I(X1^2) + X1*X2 + I(X1^X3) + X1*I(X2>0))
summary(mod0.true)
Y.decili <- quantile(Y, 0:10/10)
Y.decili
soglie  <- cut(Y, Y.decili, include.lowest=TRUE)
soglie
plot(X1, X2, col=grey(10:2/15)[soglie], pch=20, xlab="X1",ylab="X2")
mod12 <- tree::tree(Y~X1+X2)
install.packages("tree")
mod12 <- tree::tree(Y~X1+X2)
tree::partition.tree(mod12, add=TRUE)
mod12
install.packages("rpart")
mod1 <- rpart(Y ~ X1 + X2 + X3, control = rpart.control(cp = 0.0, minsplit = 10))
####### fit a regression tree on our data
library(rpart)
mod1 <- rpart(Y ~ X1 + X2 + X3, control = rpart.control(cp = 0.0, minsplit = 10))
rpart.plot::prp(mod1)
install.packages("rpart.plot")
library(rpart.plot)
rpart.plot::prp(mod1)
X3
as.factor(X3)
summary(mod1)# too long - see it on a shorter tree
?rpart.object
mod1$frame
rpart.plot::prp(mod1, extra = 1, type = 1)
#########  Data genetation
# change seetting as you prefer
n <- 100 # change to 100, 500
set.seed(26)
X1 <- round(rnorm(n), 1)
X2 <- round(rt(n, df = 6), 1) # t distribution
X3 <- rbinom(n, 1, .6) # attention to set the factors if not binary, bernoulli distribution
e <- round(rnorm(n), 1)
Y <- X1^2 - 2*X1*X2 + 2*X1^X3 - X1*I(X2>0) + e
## ordinary regression
mod0 <- lm(Y ~ X1 + X2 + X3) # --> restricting g(x) to be found in the family of linear functions
# of course this is wrong as an assumption, as the relation between X1, X2, X3 and Y is not linear
summary(mod0) # this assumes that the model is true, so when the assumption is wrong, also the answer
# this is the true regression model, assumed hierarchical.
# use : instead of * to have exactly the true model
mod0.true <- lm(Y ~ I(X1^2) + X1*X2 + I(X1^X3) + X1*I(X2>0))
summary(mod0.true)
Y.decili <- quantile(Y, 0:10/10)
Y.decili
soglie  <- cut(Y, Y.decili, include.lowest=TRUE)
soglie
plot(X1, X2, col=grey(10:2/15)[soglie], pch=20, xlab="X1",ylab="X2")
mod12 <- tree::tree(Y~X1+X2)
tree::partition.tree(mod12, add=TRUE)
mod12
####### fit a regression tree on our data
library(rpart)
library(rpart.plot)
mod1 <- rpart(Y ~ X1 + X2 + X3, control = rpart.control(cp = 0.0, minsplit = 10)) # we say to the procedure to avoid
rpart.plot::prp(mod1)
summary(mod1)# too long - see it on a shorter tree
?rpart.object
mod1$frame
mean((Y[X1<2.2] - mean(Y[X1<2.2]))^2) + mean((Y[X1>=2.2] - mean(Y[X1>=2.2]))^2)
mean((Y[X3==1] - mean(Y[X3==1]))^2) + mean((Y[X3==0] - mean(Y[X3==0]))^2)
rpart.plot::prp(mod1, extra = 1, type = 1)
mod1 <- rpart(Y ~ X1 + X2 + X3, control = rpart.control(cp = 0.0, minsplit = 30)) # we say to the procedure to avoid
rpart.plot::prp(mod1)
summary(mod1)# too long - see it on a shorter tree
?rpart.object
mod1$frame
mean((Y[X1<2.2] - mean(Y[X1<2.2]))^2) + mean((Y[X1>=2.2] - mean(Y[X1>=2.2]))^2)
mean((Y[X3==1] - mean(Y[X3==1]))^2) + mean((Y[X3==0] - mean(Y[X3==0]))^2)
rpart.plot::prp(mod1, extra = 1, type = 1)
#########  Data genetation
# change seetting as you prefer
n <- 200 # change to 100, 500
set.seed(26)
X1 <- round(rnorm(n), 1)
X2 <- round(rt(n, df = 6), 1) # t distribution
X3 <- rbinom(n, 1, .6) # attention to set the factors if not binary, bernoulli distribution
e <- round(rnorm(n), 1)
Y <- X1^2 - 2*X1*X2 + 2*X1^X3 - X1*I(X2>0) + e
## ordinary regression
mod0 <- lm(Y ~ X1 + X2 + X3) # --> restricting g(x) to be found in the family of linear functions
# of course this is wrong as an assumption, as the relation between X1, X2, X3 and Y is not linear
summary(mod0) # this assumes that the model is true, so when the assumption is wrong, also the answer
# this is the true regression model, assumed hierarchical.
# use : instead of * to have exactly the true model
mod0.true <- lm(Y ~ I(X1^2) + X1*X2 + I(X1^X3) + X1*I(X2>0))
summary(mod0.true)
Y.decili <- quantile(Y, 0:10/10)
Y.decili
soglie  <- cut(Y, Y.decili, include.lowest=TRUE)
soglie
plot(X1, X2, col=grey(10:2/15)[soglie], pch=20, xlab="X1",ylab="X2")
mod12 <- tree::tree(Y~X1+X2)
tree::partition.tree(mod12, add=TRUE)
mod12
####### fit a regression tree on our data
library(rpart)
library(rpart.plot)
mod1 <- rpart(Y ~ X1 + X2 + X3, control = rpart.control(cp = 0.0, minsplit = 30)) # we say to the procedure to avoid
rpart.plot::prp(mod1)
summary(mod1)# too long - see it on a shorter tree
?rpart.object
mod1$frame
mean((Y[X1<2.2] - mean(Y[X1<2.2]))^2) + mean((Y[X1>=2.2] - mean(Y[X1>=2.2]))^2)
mean((Y[X3==1] - mean(Y[X3==1]))^2) + mean((Y[X3==0] - mean(Y[X3==0]))^2)
rpart.plot::prp(mod1, extra = 1, type = 1)
## PRUNING
mod1$cptable # norice that this values are computed using a 10-fol CV
plotcp(mod1)
# the prooned tree has to be below the h line
abline(v = which.min(mod1$cptable[,4]), col = 'mediumpurple4') # notice trade-off accuracy-interpretability
abline(v = 3, col = 'mediumpurple1') # size 5
abline(v = 6, col = 'mediumpurple3') # size 12
abline(h = aa, col = 'mediumpurple3') # size 12
aa <- min(mod1$cptable[,4])*3.811789
abline(h = aa, col = 'mediumpurple3') # size 12
par(mfrow=c(1,2))
rsq.rpart( mod1) # plot the residual sum of squares
abline(v = mod1$cptable[6,2], col = 'mediumpurple1') # size 7
abline(v = mod1$cptable[which.min(mod1$cptable[,4]),2], col = 'mediumpurple4')
par(mfrow=c(1,1))
par(mfrow=c(1,1))
# rule of the thumb: relerror + xstd < xerror
plot(mod1$cptable[,2], mod1$cptable[,3], type="b", pch=20)
lines(mod1$cptable[,2],mod1$cptable[,3]+ mod1$cptable[,5], col="purple")
lines(mod1$cptable[,2],mod1$cptable[,4], col="maroon")
mod1$cptable[,3]+ mod1$cptable[,5] < mod1$cptable[,4]
mod1$cptable[3:5,]
# 1se rule
which.min(mod1$cptable[,4]+ mod1$cptable[,5])
## Prune the tree: according to the min xerror
mod1.pruned.long <- prune.rpart(mod1, cp = mod1$cptable[which.min(mod1$cptable[,4]),1] )
summary(mod1.pruned.long)
rpart.plot::prp(mod1.pruned.long, extra = 1, type = 1)
mod1.pruned <- prune.rpart(mod1, cp = 0.027)
summary(mod1.pruned)
rpart.plot::prp(mod1.pruned, extra = 1, type = 1)
# extract dummies function
library(rpart.utils)
rpart.lists(mod1.pruned)
rpart.subrules.table(mod1.pruned)
install.packages("rpart.utils")
brary(rpart)
library(MASS)
data(Boston)
head(Boston)
lm.out <- lm(medv ~ ptratio, data = Boston)
plot(medv ~ ptratio, data = Boston)
abline(lm.out, col="red")
fit <- rpart(medv ~ ptratio, data = Boston)
fit.pruned.tree <-prune(fit, cp = 0.05)
plot(fit.pruned.tree)
text(fit.pruned.tree, use.n = TRUE)
fit.pruned.tree
plot(medv ~ ptratio, data = Boston)
plot(fit.pruned.tree)
text(fit.pruned.tree, use.n = TRUE)
y
y
library(gRbase)
library(gRain)
library(Rgraphviz)
#### How to construct a DAG
dag0 <- dag(~b*a,~c*a,~b*c,~d*b,~e*a,~d*e,~f*c,~f*b)
Rgraphviz::plot(dag0, cex.main = 0.5) # note how the script works: ~c * a * b = c depends on both a and b
dag0
edgeList(dag0) # when the class of edges is "oriented", every edge is in the form of <parent> <child>
as.adjMAT(dag0) # adjacency matrix, on rows = parents, on columns = child
# these two functions identify the parents or the children of a specified node
parents("d", dag0)
children("a", dag0)
# moralized graph
ug0 <- moralize(dag0)
iplot(ug0)
# functions to get edges and cliques
edgeList(ug0)
getCliques(ug0)
# check: b ind e?
ancestralSet(c("a"), dag0)
iplot(ancestralGraph(c("b", "e"), dag0))
# check: b ind e?
ancestralSet(c("c"), dag0)
parents(c("d", "b"), dag0)
library(gRbase)
library(gRain)
library(Rgraphviz)
#### How to construct a DAG
dag0 <- dag(~b*a,~c*a,~b*c,~d*b,~e*a,~d*e,~f*c,~f*b)
Rgraphviz::plot(dag0, cex.main = 0.5) # note how the script works: ~c * a * b = c depends on both a and b
dag0
edgeList(dag0) # when the class of edges is "oriented", every edge is in the form of <parent> <child>
as.adjMAT(dag0) # adjacency matrix, on rows = parents, on columns = child
# these two functions identify the parents or the children of a specified node
parents("d", dag0)
children("a", dag0)
# moralized graph
ug0 <- moralize(dag0)
iplot(ug0)
# functions to get edges and cliques
edgeList(ug0)
getCliques(ug0)
# check: b ind e?
ancestralSet(c("b", "e"), dag0)
iplot(ancestralGraph(c("b", "e"), dag0))
iplot(moralize(ancestralGraph(c("b", "e"), dag0))) # --> no
# check: (d,e) ind b | c ?
ancestralSet(c("b", "e", "d", "c"), dag0)
(b, c) ind e
# check: (d,e) ind b | c ?
ancestralSet(c("b", "e", "d", "c"), dag0)
iplot(ancestralGraph(c("b", "e", "d", "c"), dag0))
iplot(moralize(ancestralGraph(c("b", "e", "d", "c"), dag0)))
# check: b ind e?
ancestralSet(c("b", "e"), dag0)
iplot(ancestralGraph(c("b", "e"), dag0))
iplot(moralize(ancestralGraph(c("b", "e"), dag0))) # --> no
# check: b ind e?
ancestralSet(c("b", "e"), dag0)
# check: b ind e?
ancestralSet(c("b", "e"), dag0)
iplot(ancestralGraph(c("b", "e"), dag0))
iplot(moralize(ancestralGraph(c("b", "e"), dag0))) # --> no
# check: a ind d
ancestralSet(c("a", "d"), dag0)
iplot(ancestralGraph(c("a", "d"), dag0))
iplot(moralize(ancestralGraph(c("a", "d"), dag0)))
# check: b ind e?
ancestralSet(c("b", "e"), dag0)
iplot(ancestralGraph(c("b", "e"), dag0))
iplot(moralize(ancestralGraph(c("b", "e"), dag0))) # --> no
library(gRbase)
library(gRain)
library(Rgraphviz)
#### How to construct a DAG
dag0 <- dag(~b*a,~c*a,~b*c,~d*b,~e*a,~d*e,~f*c,~f*b)
Rgraphviz::plot(dag0, cex.main = 0.5) # note how the script works: ~c * a * b = c depends on both a and b
dag0
edgeList(dag0) # when the class of edges is "oriented", every edge is in the form of <parent> <child>
as.adjMAT(dag0) # adjacency matrix, on rows = parents, on columns = child
# these two functions identify the parents or the children of a specified node
parents("d", dag0)
children("a", dag0)
# moralized graph
ug0 <- moralize(dag0)
iplot(ug0)
# functions to get edges and cliques
edgeList(ug0)
getCliques(ug0)
# check: b ind e?
ancestralSet(c("b", "e"), dag0)
iplot(ancestralGraph(c("b", "e"), dag0))
iplot(moralize(ancestralGraph(c("b", "e"), dag0))) # --> no
######################  CLASSIFICATION TREES
library(rpart)
par(mfrow = c(1,1))
data(kyphosis)
head(kyphosis)
fit1 <- rpart(Kyphosis ~ ., data = kyphosis,
parms=list(split = "information"))
fit1
summary(fit1)
plot(fit1)
rpart.plot::rpart.plot(fit1)
install.packages("randomForest")
require(randomForest)
require(MASS)
set.seed(101)
dim(Boston)
train=sample(1:nrow(Boston),300)
?Boston
rf.boston = randomForest(medv~. ,data = Boston, subset = train)
rf.boston
.err=double(13)
test.err=double(13)
for(mtry in 1:13){
fit=randomForest(medv~.,data=Boston,subset=train,mtry=mtry,ntree=400)
oob.err[mtry]=fit$mse[400]
pred=predict(fit,Boston[-train,])
test.err[mtry]=with(Boston[-train,],mean((medv-pred)^2))
cat(mtry," ")
}
oob.err=double(13)
test.err=double(13)
for(mtry in 1:13){
fit=randomForest(medv~.,data=Boston,subset=train,mtry=mtry,ntree=400)
oob.err[mtry]=fit$mse[400]
pred=predict(fit,Boston[-train,])
test.err[mtry]=with(Boston[-train,],mean((medv-pred)^2))
cat(mtry," ")
}
matplot(1:mtry,cbind(test.err,oob.err),pch=19,col=c("red","blue"),type="b",ylab="Mean Squared Error")
legend("topright",legend=c("OOB","Test"),pch=19,col=c("red","blue"))
require(gbm)
boost.boston=gbm(medv~.,data=Boston[train,],distribution="gaussian",n.trees=10000,shrinkage=0.01,interaction.depth=4)
install.packages("gbm")
require(gbm)
boost.boston=gbm(medv~.,data=Boston[train,],distribution="gaussian",n.trees=10000,shrinkage=0.01,interaction.depth=4)
summary(boost.boston)
##########################################
# BART
##########################################
rm(list=ls())
dyn.load('/Library/Java/JavaVirtualMachines/jdk1.8.0_45.jdk/Contents/Home/jre/lib/server/libjvm.dylib')
options(java.parameters="-Xmx5000m") # must be set initially
library(bartMachine)
install.packages("bartMachine")
set_bart_machine_num_cores(4)
##################################################################################
#Now we fit the models using the lasso estimator
#in glmnet default `alpha=1`
library(ISLR)
library(glmnet)
Hitters <- na.omit(Hitters)
x = model.matrix(Salary ~ . -1, data = Hitters)
y = Hitters$Salary
################# Elastic net
set.seed(26)
cv.EN <- cv.glmnet(x, y, alpha = 0.5, nfolds = 10) # alpha is another tuning parameter to be set
cv.EN
plot(cv.EN)
cv.EN_2 <- cv.glmnet(x[train,], y[train], nfolds = 10, alpha = 0.5)
cv.EN_2
plot(cv.EN_2)
ENpred.min <- predict(cv.EN_2, s = cv.EN_2$lambda.min, newx = x[-train,])
######## Ridge Regression and Lasso
library(glmnet)
library(ISLR)
# data import
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
data <- read.csv("housing.csv", header = F)
column_names = c('CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV')
colnames(data) <- column_names
head(data)
mod0 <- lm(MEDV ~ . -1, data = data)
summary(mod0)
